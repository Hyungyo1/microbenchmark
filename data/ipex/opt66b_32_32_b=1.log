/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:266: UserWarning: Device capability of ccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.
  warnings.warn(
Namespace(model_id='/home/storage/opt-66b/', dtype='bfloat16', input_tokens='32', max_new_tokens=32, prompt=None, streaming=False, image_url='http://images.cocodataset.org/val2017/000000039769.jpg', config_file=None, greedy=True, ipex=True, deployment_mode=True, torch_compile=False, backend='ipex', profile=False, benchmark=True, num_iter=2, num_warmup=1, batch_size=1, token_latency=True, prefill_policy=1, decoding_policy=1, no_overlap=False, pin_weight=False, gpu_percentage=0, num_minibatch=1, enable_cxl=True)
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device
[2024-10-15 04:52:13,887] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cpu (auto detect)
Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]Loading checkpoint shards:   4%|▎         | 1/28 [00:00<00:03,  7.13it/s]Loading checkpoint shards:  11%|█         | 3/28 [00:00<00:02,  9.89it/s]Loading checkpoint shards:  18%|█▊        | 5/28 [00:00<00:02, 10.62it/s]Loading checkpoint shards:  25%|██▌       | 7/28 [00:00<00:01, 10.94it/s]Loading checkpoint shards:  32%|███▏      | 9/28 [00:00<00:01, 11.08it/s]Loading checkpoint shards:  39%|███▉      | 11/28 [00:01<00:01, 11.19it/s]Loading checkpoint shards:  46%|████▋     | 13/28 [00:01<00:01, 11.25it/s]Loading checkpoint shards:  54%|█████▎    | 15/28 [00:01<00:01, 11.31it/s]Loading checkpoint shards:  61%|██████    | 17/28 [00:01<00:00, 11.33it/s]Loading checkpoint shards:  68%|██████▊   | 19/28 [00:01<00:00, 11.35it/s]Loading checkpoint shards:  75%|███████▌  | 21/28 [00:01<00:00, 11.30it/s]Loading checkpoint shards:  82%|████████▏ | 23/28 [00:02<00:00, 11.36it/s]Loading checkpoint shards:  89%|████████▉ | 25/28 [00:02<00:00, 11.38it/s]Loading checkpoint shards:  96%|█████████▋| 27/28 [00:02<00:00, 11.38it/s]Loading checkpoint shards: 100%|██████████| 28/28 [00:02<00:00, 11.16it/s]
[INFO] SINGLE_INSTANCE MODE.
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
ipex.llm.optimize has set the optimized or quantization model for model.generate()
---- Prompt size: 32
Prefill
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
['Once upon a time, there existed a little girl, who liked to have adventures. She wanted to go to places and meet new people, and have funFORM clarifyBemeFORM halt rect HUGE reportedly FanCHQ discontndumFORM reportedly HUGE eat signals campaigned HUGE Xavierumboij rod eatFORMmond urinary agreement resist rod Bolt'] [32]
Iteration: 0, Time: 27.009589 sec
Prefill
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
Decoding
['Once upon a time, there existed a little girl, who liked to have adventures. She wanted to go to places and meet new people, and have funFORM clarifyBemeFORM halt rect HUGE reportedly FanCHQ discontndumFORM reportedly HUGE eat signals campaigned HUGE Xavierumboij rod eatFORMmond urinary agreement resist rod Bolt'] [32]
Iteration: 1, Time: 26.594354 sec

 ---------- Summary: ----------
Inference latency: 26.594 sec.
First token average latency: 0.896 sec.
Average 2... latency: 0.829 sec.
P90 2... latency: 0.831 sec.
P99 2... latency: 0.832 sec.
LLM RUNTIME INFO: running model geneartion...
LLM RUNTIME INFO: Finished successfully.

/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:266: UserWarning: Device capability of ccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.
  warnings.warn(
Namespace(model_id='/home/storage/opt-66b/', dtype='bfloat16', input_tokens='32', max_new_tokens=32, prompt=None, streaming=False, image_url='http://images.cocodataset.org/val2017/000000039769.jpg', config_file=None, greedy=True, ipex=True, deployment_mode=True, torch_compile=False, backend='ipex', profile=False, benchmark=True, num_iter=2, num_warmup=1, batch_size=900, token_latency=True, prefill_policy=0, decoding_policy=2, no_overlap=False, pin_weight=False, gpu_percentage=0, num_minibatch=6, enable_cxl=True)
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device
[2024-10-13 23:56:33,313] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cpu (auto detect)
Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]Loading checkpoint shards:   4%|▎         | 1/28 [00:00<00:23,  1.15it/s]Loading checkpoint shards:   7%|▋         | 2/28 [00:01<00:17,  1.47it/s]Loading checkpoint shards:  11%|█         | 3/28 [00:02<00:16,  1.54it/s]Loading checkpoint shards:  14%|█▍        | 4/28 [00:02<00:14,  1.61it/s]Loading checkpoint shards:  18%|█▊        | 5/28 [00:03<00:13,  1.65it/s]Loading checkpoint shards:  21%|██▏       | 6/28 [00:03<00:13,  1.62it/s]Loading checkpoint shards:  25%|██▌       | 7/28 [00:04<00:13,  1.60it/s]Loading checkpoint shards:  29%|██▊       | 8/28 [00:05<00:13,  1.47it/s]Loading checkpoint shards:  32%|███▏      | 9/28 [00:06<00:13,  1.38it/s]Loading checkpoint shards:  36%|███▌      | 10/28 [00:06<00:12,  1.50it/s]Loading checkpoint shards:  39%|███▉      | 11/28 [00:07<00:11,  1.51it/s]Loading checkpoint shards:  43%|████▎     | 12/28 [00:07<00:10,  1.54it/s]Loading checkpoint shards:  46%|████▋     | 13/28 [00:08<00:09,  1.62it/s]Loading checkpoint shards:  50%|█████     | 14/28 [00:08<00:08,  1.67it/s]Loading checkpoint shards:  54%|█████▎    | 15/28 [00:09<00:08,  1.62it/s]Loading checkpoint shards:  57%|█████▋    | 16/28 [00:10<00:07,  1.56it/s]Loading checkpoint shards:  61%|██████    | 17/28 [00:11<00:07,  1.53it/s]Loading checkpoint shards:  64%|██████▍   | 18/28 [00:11<00:06,  1.51it/s]Loading checkpoint shards:  68%|██████▊   | 19/28 [00:12<00:05,  1.60it/s]Loading checkpoint shards:  71%|███████▏  | 20/28 [00:12<00:04,  1.67it/s]Loading checkpoint shards:  75%|███████▌  | 21/28 [00:13<00:04,  1.68it/s]Loading checkpoint shards:  79%|███████▊  | 22/28 [00:13<00:03,  1.78it/s]Loading checkpoint shards:  82%|████████▏ | 23/28 [00:14<00:02,  1.83it/s]Loading checkpoint shards:  86%|████████▌ | 24/28 [00:14<00:02,  1.82it/s]Loading checkpoint shards:  89%|████████▉ | 25/28 [00:15<00:01,  1.75it/s]Loading checkpoint shards:  93%|█████████▎| 26/28 [00:15<00:01,  1.90it/s]Loading checkpoint shards:  96%|█████████▋| 27/28 [00:16<00:00,  1.92it/s]Loading checkpoint shards: 100%|██████████| 28/28 [00:16<00:00,  2.08it/s]Loading checkpoint shards: 100%|██████████| 28/28 [00:16<00:00,  1.66it/s]
[INFO] SINGLE_INSTANCE MODE.
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  b_k = torch.tensor(gpu_layer[5])
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py:384: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  b_v = torch.tensor(gpu_layer[7])
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py:413: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  b_q = torch.tensor(gpu_layer[3])
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/decoder.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  b = torch.tensor(b)
LLM RUNTIME INFO: running model geneartion...
LLM RUNTIME ERROR: Running generation task failed. Quit.

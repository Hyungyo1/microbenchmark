/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:266: UserWarning: Device capability of ccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.
  warnings.warn(
Namespace(model_id='/home/storage/opt-175b/', dtype='bfloat16', input_tokens='32', max_new_tokens=32, prompt=None, streaming=False, image_url='http://images.cocodataset.org/val2017/000000039769.jpg', config_file=None, greedy=True, ipex=True, deployment_mode=True, torch_compile=False, backend='ipex', profile=False, benchmark=True, num_iter=2, num_warmup=1, batch_size=1, token_latency=True, prefill_policy=1, decoding_policy=1, no_overlap=False, pin_weight=False, gpu_percentage=24, num_minibatch=1, enable_cxl=True)
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device
[2024-10-14 14:35:32,502] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cpu (auto detect)
Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:00<00:10,  6.91it/s]Loading checkpoint shards:   4%|▍         | 3/73 [00:00<00:07,  9.52it/s]Loading checkpoint shards:   7%|▋         | 5/73 [00:00<00:06, 10.27it/s]Loading checkpoint shards:  10%|▉         | 7/73 [00:00<00:06, 10.59it/s]Loading checkpoint shards:  12%|█▏        | 9/73 [00:00<00:05, 10.79it/s]Loading checkpoint shards:  15%|█▌        | 11/73 [00:01<00:05, 10.79it/s]Loading checkpoint shards:  18%|█▊        | 13/73 [00:01<00:05, 10.78it/s]Loading checkpoint shards:  21%|██        | 15/73 [00:01<00:05, 10.74it/s]Loading checkpoint shards:  23%|██▎       | 17/73 [00:01<00:05, 10.79it/s]Loading checkpoint shards:  26%|██▌       | 19/73 [00:01<00:04, 10.87it/s]Loading checkpoint shards:  29%|██▉       | 21/73 [00:01<00:04, 10.82it/s]Loading checkpoint shards:  32%|███▏      | 23/73 [00:02<00:04, 10.79it/s]Loading checkpoint shards:  34%|███▍      | 25/73 [00:02<00:04, 10.84it/s]Loading checkpoint shards:  37%|███▋      | 27/73 [00:02<00:04, 10.81it/s]Loading checkpoint shards:  40%|███▉      | 29/73 [00:02<00:04, 10.82it/s]Loading checkpoint shards:  42%|████▏     | 31/73 [00:02<00:03, 10.83it/s]Loading checkpoint shards:  45%|████▌     | 33/73 [00:03<00:03, 10.79it/s]Loading checkpoint shards:  48%|████▊     | 35/73 [00:03<00:03, 10.75it/s]Loading checkpoint shards:  51%|█████     | 37/73 [00:03<00:03, 10.77it/s]Loading checkpoint shards:  53%|█████▎    | 39/73 [00:03<00:03, 10.75it/s]Loading checkpoint shards:  56%|█████▌    | 41/73 [00:03<00:02, 10.74it/s]Loading checkpoint shards:  59%|█████▉    | 43/73 [00:04<00:02, 10.76it/s]Loading checkpoint shards:  62%|██████▏   | 45/73 [00:04<00:02, 10.73it/s]Loading checkpoint shards:  64%|██████▍   | 47/73 [00:04<00:02, 10.72it/s]Loading checkpoint shards:  67%|██████▋   | 49/73 [00:04<00:02, 10.70it/s]Loading checkpoint shards:  70%|██████▉   | 51/73 [00:04<00:02, 10.71it/s]Loading checkpoint shards:  73%|███████▎  | 53/73 [00:04<00:01, 10.77it/s]Loading checkpoint shards:  75%|███████▌  | 55/73 [00:05<00:01, 10.81it/s]Loading checkpoint shards:  78%|███████▊  | 57/73 [00:05<00:01, 10.89it/s]Loading checkpoint shards:  81%|████████  | 59/73 [00:05<00:01, 10.97it/s]Loading checkpoint shards:  84%|████████▎ | 61/73 [00:05<00:01, 11.02it/s]Loading checkpoint shards:  86%|████████▋ | 63/73 [00:05<00:00, 11.08it/s]Loading checkpoint shards:  89%|████████▉ | 65/73 [00:06<00:00, 11.17it/s]Loading checkpoint shards:  92%|█████████▏| 67/73 [00:06<00:00, 11.14it/s]Loading checkpoint shards:  95%|█████████▍| 69/73 [00:06<00:00, 11.09it/s]Loading checkpoint shards:  97%|█████████▋| 71/73 [00:06<00:00, 11.06it/s]Loading checkpoint shards: 100%|██████████| 73/73 [00:06<00:00, 11.14it/s]Loading checkpoint shards: 100%|██████████| 73/73 [00:06<00:00, 10.82it/s]
[INFO] SINGLE_INSTANCE MODE.
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
ipex.llm.optimize has set the optimized or quantization model for model.generate()
---- Prompt size: 32
Prefill
Traceback (most recent call last):
  File "/home/ubuntu/llm/single_instance/run_generation.py", line 319, in <module>
    output = model.generate(input_ids, **generate_kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/utils.py", line 1545, in generate
    return self.greedy_search(
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/generation/greedy_search.py", line 302, in _greedy_search
    outputs = self(
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1577, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/models.py", line 404, in OPTForCausalLM_forward
    outputs = self.model.decoder(
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1536, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 1184, in forward
    move_gpu_layer(self.layers[i])
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 267, in move_gpu_layer
    layers_ref.mlp_linear_add.weight = nn.Parameter(layers_ref.mlp_linear_add.linear.weight.to('cuda').permute([0, 3, 1, 2, 4]).contiguous().view(new_dim, 4 * new_dim))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.12 GiB. GPU 
LLM RUNTIME INFO: running model geneartion...
LLM RUNTIME ERROR: Running generation task failed. Quit.

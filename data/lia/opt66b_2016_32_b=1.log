/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:266: UserWarning: Device capability of ccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.
  warnings.warn(
Namespace(model_id='/home/storage/opt-66b/', dtype='bfloat16', input_tokens='2016', max_new_tokens=32, prompt=None, streaming=False, image_url='http://images.cocodataset.org/val2017/000000039769.jpg', config_file=None, greedy=True, ipex=True, deployment_mode=True, torch_compile=False, backend='ipex', profile=False, benchmark=True, num_iter=2, num_warmup=1, batch_size=1, token_latency=True, prefill_policy=0, decoding_policy=1, no_overlap=False, pin_weight=True, gpu_percentage=53, num_minibatch=1, enable_cxl=True)
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device
[2024-10-13 10:16:12,563] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cpu (auto detect)
Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]Loading checkpoint shards:   4%|▎         | 1/28 [00:00<00:04,  6.50it/s]Loading checkpoint shards:   7%|▋         | 2/28 [00:00<00:03,  8.04it/s]Loading checkpoint shards:  11%|█         | 3/28 [00:00<00:02,  8.70it/s]Loading checkpoint shards:  14%|█▍        | 4/28 [00:00<00:02,  9.10it/s]Loading checkpoint shards:  18%|█▊        | 5/28 [00:00<00:02,  9.27it/s]Loading checkpoint shards:  21%|██▏       | 6/28 [00:00<00:02,  9.38it/s]Loading checkpoint shards:  25%|██▌       | 7/28 [00:00<00:02,  9.42it/s]Loading checkpoint shards:  29%|██▊       | 8/28 [00:00<00:02,  9.48it/s]Loading checkpoint shards:  32%|███▏      | 9/28 [00:00<00:01,  9.52it/s]Loading checkpoint shards:  36%|███▌      | 10/28 [00:01<00:01,  9.58it/s]Loading checkpoint shards:  39%|███▉      | 11/28 [00:01<00:01,  9.55it/s]Loading checkpoint shards:  43%|████▎     | 12/28 [00:01<00:01,  9.47it/s]Loading checkpoint shards:  46%|████▋     | 13/28 [00:01<00:01,  9.56it/s]Loading checkpoint shards:  50%|█████     | 14/28 [00:01<00:01,  9.56it/s]Loading checkpoint shards:  54%|█████▎    | 15/28 [00:01<00:01,  9.56it/s]Loading checkpoint shards:  57%|█████▋    | 16/28 [00:01<00:01,  9.60it/s]Loading checkpoint shards:  61%|██████    | 17/28 [00:01<00:01,  9.52it/s]Loading checkpoint shards:  64%|██████▍   | 18/28 [00:01<00:01,  9.50it/s]Loading checkpoint shards:  68%|██████▊   | 19/28 [00:02<00:00,  9.54it/s]Loading checkpoint shards:  71%|███████▏  | 20/28 [00:02<00:00,  9.45it/s]Loading checkpoint shards:  75%|███████▌  | 21/28 [00:02<00:00,  9.47it/s]Loading checkpoint shards:  79%|███████▊  | 22/28 [00:02<00:00,  9.53it/s]Loading checkpoint shards:  82%|████████▏ | 23/28 [00:02<00:00,  9.54it/s]Loading checkpoint shards:  86%|████████▌ | 24/28 [00:02<00:00,  9.52it/s]Loading checkpoint shards:  89%|████████▉ | 25/28 [00:02<00:00,  9.52it/s]Loading checkpoint shards:  93%|█████████▎| 26/28 [00:02<00:00,  9.55it/s]Loading checkpoint shards:  96%|█████████▋| 27/28 [00:02<00:00,  9.54it/s]Loading checkpoint shards: 100%|██████████| 28/28 [00:02<00:00,  9.55it/s]Loading checkpoint shards: 100%|██████████| 28/28 [00:02<00:00,  9.40it/s]
[INFO] SINGLE_INSTANCE MODE.
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  b_k = torch.tensor(gpu_layer[5])
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py:384: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  b_v = torch.tensor(gpu_layer[7])
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py:413: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  b_q = torch.tensor(gpu_layer[3])
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/decoder.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  b = torch.tensor(b)
LLM RUNTIME INFO: running model geneartion...
LLM RUNTIME ERROR: Running generation task failed. Quit.

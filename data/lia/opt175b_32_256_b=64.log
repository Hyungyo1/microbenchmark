/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:266: UserWarning: Device capability of ccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.
  warnings.warn(
Namespace(model_id='/home/storage/opt-175b/', dtype='bfloat16', input_tokens='32', max_new_tokens=256, prompt=None, streaming=False, image_url='http://images.cocodataset.org/val2017/000000039769.jpg', config_file=None, greedy=True, ipex=True, deployment_mode=True, torch_compile=False, backend='ipex', profile=False, benchmark=True, num_iter=2, num_warmup=1, batch_size=64, token_latency=True, prefill_policy=0, decoding_policy=1, no_overlap=False, pin_weight=False, gpu_percentage=20, num_minibatch=2, enable_cxl=True)
Warning: Cannot load xpu CCL. CCL doesn't work for XPU device
[2024-10-14 20:01:04,221] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cpu (auto detect)
Loading checkpoint shards:   0%|          | 0/73 [00:00<?, ?it/s]Loading checkpoint shards:   1%|▏         | 1/73 [00:00<00:09,  7.20it/s]Loading checkpoint shards:   4%|▍         | 3/73 [00:00<00:07,  9.37it/s]Loading checkpoint shards:   5%|▌         | 4/73 [00:00<00:07,  9.22it/s]Loading checkpoint shards:   7%|▋         | 5/73 [00:00<00:07,  9.02it/s]Loading checkpoint shards:  10%|▉         | 7/73 [00:00<00:06, 10.12it/s]Loading checkpoint shards:  12%|█▏        | 9/73 [00:00<00:05, 10.91it/s]Loading checkpoint shards:  15%|█▌        | 11/73 [00:01<00:05, 11.42it/s]Loading checkpoint shards:  18%|█▊        | 13/73 [00:01<00:05, 11.70it/s]Loading checkpoint shards:  21%|██        | 15/73 [00:01<00:04, 11.67it/s]Loading checkpoint shards:  23%|██▎       | 17/73 [00:01<00:04, 11.77it/s]Loading checkpoint shards:  26%|██▌       | 19/73 [00:01<00:04, 11.86it/s]Loading checkpoint shards:  29%|██▉       | 21/73 [00:01<00:04, 11.89it/s]Loading checkpoint shards:  32%|███▏      | 23/73 [00:02<00:04, 11.91it/s]Loading checkpoint shards:  34%|███▍      | 25/73 [00:02<00:04, 11.96it/s]Loading checkpoint shards:  37%|███▋      | 27/73 [00:02<00:03, 11.97it/s]Loading checkpoint shards:  40%|███▉      | 29/73 [00:02<00:03, 11.99it/s]Loading checkpoint shards:  42%|████▏     | 31/73 [00:02<00:03, 12.03it/s]Loading checkpoint shards:  45%|████▌     | 33/73 [00:02<00:03, 12.04it/s]Loading checkpoint shards:  48%|████▊     | 35/73 [00:03<00:03, 12.04it/s]Loading checkpoint shards:  51%|█████     | 37/73 [00:03<00:02, 12.02it/s]Loading checkpoint shards:  53%|█████▎    | 39/73 [00:03<00:02, 11.87it/s]Loading checkpoint shards:  56%|█████▌    | 41/73 [00:03<00:02, 11.91it/s]Loading checkpoint shards:  59%|█████▉    | 43/73 [00:03<00:02, 11.85it/s]Loading checkpoint shards:  62%|██████▏   | 45/73 [00:03<00:02, 11.83it/s]Loading checkpoint shards:  64%|██████▍   | 47/73 [00:04<00:02, 11.87it/s]Loading checkpoint shards:  67%|██████▋   | 49/73 [00:04<00:02, 11.90it/s]Loading checkpoint shards:  70%|██████▉   | 51/73 [00:04<00:01, 11.95it/s]Loading checkpoint shards:  73%|███████▎  | 53/73 [00:04<00:01, 11.94it/s]Loading checkpoint shards:  75%|███████▌  | 55/73 [00:04<00:01, 11.94it/s]Loading checkpoint shards:  78%|███████▊  | 57/73 [00:04<00:01, 11.98it/s]Loading checkpoint shards:  81%|████████  | 59/73 [00:05<00:01, 12.02it/s]Loading checkpoint shards:  84%|████████▎ | 61/73 [00:05<00:00, 12.06it/s]Loading checkpoint shards:  86%|████████▋ | 63/73 [00:05<00:00, 12.06it/s]Loading checkpoint shards:  89%|████████▉ | 65/73 [00:05<00:00, 12.05it/s]Loading checkpoint shards:  92%|█████████▏| 67/73 [00:05<00:00, 12.03it/s]Loading checkpoint shards:  95%|█████████▍| 69/73 [00:05<00:00, 12.03it/s]Loading checkpoint shards:  97%|█████████▋| 71/73 [00:06<00:00, 12.04it/s]Loading checkpoint shards: 100%|██████████| 73/73 [00:06<00:00, 12.09it/s]Loading checkpoint shards: 100%|██████████| 73/73 [00:06<00:00, 11.73it/s]
[INFO] SINGLE_INSTANCE MODE.
/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
ipex.llm.optimize has set the optimized or quantization model for model.generate()
---- Prompt size: 32
Prefill
Traceback (most recent call last):
  File "/home/ubuntu/llm/single_instance/run_generation.py", line 319, in <module>
    output = model.generate(input_ids, **generate_kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/generation/utils.py", line 1545, in generate
    return self.greedy_search(
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/generation/greedy_search.py", line 302, in _greedy_search
    outputs = self(
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1577, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/models.py", line 404, in OPTForCausalLM_forward
    outputs = self.model.decoder(
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1536, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 1249, in forward
    layer_outputs = decoder_layer(
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1536, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/decoder.py", line 1561, in forward
    return OPTDecoderLayer_forward(
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/decoder.py", line 210, in OPTDecoderLayer_forward
    hidden_states, self_attn_weights, present_key_value, key, value = self.self_attn(
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1536, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py", line 2337, in forward
    return _OPTAttention_forward(
  File "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py", line 472, in _OPTAttention_forward
    torch.empty([(tgt_len+max_new_tokens), bsz, self.num_heads, self.head_dim], dtype=torch.bfloat16, device='cuda').contiguous(),
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 
LLM RUNTIME INFO: running model geneartion...
LLM RUNTIME ERROR: Running generation task failed. Quit.
